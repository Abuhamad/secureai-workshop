{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Federated Learning**\n",
    "\n",
    "#### **Workshop Objective:**\n",
    "\n",
    "This workshop aims to train a federated learning model with 100 clients, using differential privacy techniques to ensure data security.\n",
    "\n",
    "**Federated Learning** allows multiple devices (clients) to train a shared machine learning model while keeping their individual data decentralized. Instead of sending raw data to a central server, clients train the model locally and only share updates (model parameters) with the server.\n",
    "\n",
    "**Differential Privacy** adds mathematical noise to these updates, ensuring that individual data points cannot be inferred, even by a determined adversary. This provides a strong layer of privacy protection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Required Libraries:\n",
    "\n",
    "* Install essential libraries like `torch` for model development and opacus for differential privacy integration.\n",
    "* `Opacus` is a library that integrates seamlessly with PyTorch to enable differential privacy in machine learning models, ensuring the training process adheres to privacy-preserving techniques.\n",
    "* We will also install other necessary libraries to support federated learning and data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from datetime import date\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn, tanh\n",
    "from torch.nn.functional import relu, softmax, max_pool2d\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import opacus\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parameters and Variables:\n",
    "\n",
    "Set up all the necessary parameters for loading, training, and managing the models. These include learning rates, batch sizes, privacy budgets, and other configurations for federated learning and differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_NAME = 'mnist'\n",
    "root = './data'\n",
    "NUM_CLIENTS = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASES_PER_CLIENT = 10 \n",
    "NUM_CLASSES = 10\n",
    "LEARNING_RATE_DIS = 2e-1\n",
    "EPOCHS = 1\n",
    "ROUNDS = 80\n",
    "sample_rate = 1\n",
    "MODE = \"LDP\"\n",
    "target_epsilon = 8\n",
    "mp_bs = 64\n",
    "target_delta = 1e-3\n",
    "\n",
    "user_param = {'disc_lr': LEARNING_RATE_DIS, 'epochs': EPOCHS} \n",
    "user_param['rounds'] = ROUNDS\n",
    "user_param['target_epsilon'] = target_epsilon\n",
    "user_param['target_delta'] = target_delta\n",
    "user_param['sr'] = sample_rate\n",
    "user_param['mp_bs'] = mp_bs\n",
    "\n",
    "server_param = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset:\n",
    "\n",
    "* Load the widely-used `MNIST10 dataset`, which consists of grayscale images of handwritten digits (0-9). Each image is 28x28 pixels and belongs to one of 10 classes.\n",
    "* Implement helper functions to divide the dataset among the 100 clients, ensuring each client receives a nearly equal number of samples from each class, to maintain balanced data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(data_name, dataroot, preprocess = None): \n",
    "    if data_name == 'mnist':\n",
    "        normalization = transforms.Normalize((0.5,), (0.5,))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalization])\n",
    "        data_obj = MNIST\n",
    "    elif data_name == 'fashionmnist':\n",
    "        normalization = transforms.Normalize((0.5,), (0.5,))\n",
    "        transform = transforms.Compose([transforms.ToTensor(),  normalization])\n",
    "        data_obj = FashionMNIST\n",
    "    else:\n",
    "        raise ValueError(\"choose data_name from ['mnist', 'fashionmnist']\")\n",
    "\n",
    "\n",
    "    train_set = data_obj(dataroot, train=True, transform=transform, download=True)\n",
    "    test_set = data_obj(dataroot, train=False, transform=transform)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_classes_samples(dataset):\n",
    "    \"\"\"\n",
    "    extracts info about certain dataset\n",
    "    :param dataset: pytorch dataset object\n",
    "    :return: dataset info number of classes, number of samples, list of labels\n",
    "    \"\"\"\n",
    "    # ---------------#\n",
    "    # Extract labels #\n",
    "    # ---------------#\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        if isinstance(dataset.dataset.targets, list):\n",
    "            data_labels_list = np.array(dataset.dataset.targets)[dataset.indices]\n",
    "        else:\n",
    "            data_labels_list = dataset.dataset.targets[dataset.indices]\n",
    "    else:\n",
    "        if isinstance(dataset.targets, list):\n",
    "            data_labels_list = np.array(dataset.targets)\n",
    "        else:\n",
    "            data_labels_list = dataset.targets\n",
    "    classes, num_samples = np.unique(data_labels_list, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "    return num_classes, num_samples, data_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_classes_per_node(dataset, num_users, classes_per_user=2, high_prob=0.6, low_prob=0.4):\n",
    "    \"\"\"\n",
    "    creates the data distribution of each client\n",
    "    :param dataset: pytorch dataset object\n",
    "    :param num_users: number of clients\n",
    "    :param classes_per_user: number of classes assigned to each client\n",
    "    :param high_prob: highest prob sampled\n",
    "    :param low_prob: lowest prob sampled\n",
    "    :return: dictionary mapping between classes and proportions, each entry refers to other client\n",
    "    \"\"\"\n",
    "    num_classes, num_samples, _ = get_num_classes_samples(dataset)\n",
    "\n",
    "    # -------------------------------------------#\n",
    "    # Divide classes + num samples for each user #\n",
    "    # -------------------------------------------#\n",
    "    # print(num_classes)\n",
    "    assert (classes_per_user * num_users) % num_classes == 0, \"equal classes appearance is needed\"\n",
    "    count_per_class = (classes_per_user * num_users) // num_classes\n",
    "    class_dict = {}\n",
    "    for i in range(num_classes):\n",
    "        probs=np.array([1]*count_per_class)\n",
    "        probs_norm = (probs / probs.sum()).tolist()\n",
    "        class_dict[i] = {'count': count_per_class, 'prob': probs_norm}\n",
    "    # -------------------------------------#\n",
    "    # Assign each client with data indexes #\n",
    "    # -------------------------------------#\n",
    "    class_partitions = defaultdict(list)\n",
    "    for i in range(num_users):\n",
    "        c = []\n",
    "        for _ in range(classes_per_user):\n",
    "            class_counts = [class_dict[i]['count'] for i in range(num_classes)]\n",
    "            max_class_counts = np.where(np.array(class_counts) == max(class_counts))[0]\n",
    "            max_class_counts = np.setdiff1d(max_class_counts, np.array(c))\n",
    "            c.append(np.random.choice(max_class_counts))\n",
    "            class_dict[c[-1]]['count'] -= 1\n",
    "        class_partitions['class'].append(c)\n",
    "        class_partitions['prob'].append([class_dict[i]['prob'].pop() for i in c])\n",
    "    return class_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_classes_per_node(dataset, num_users, classes_per_user=2, high_prob=0.6, low_prob=0.4):\n",
    "    \"\"\"\n",
    "    creates the data distribution of each client\n",
    "    :param dataset: pytorch dataset object\n",
    "    :param num_users: number of clients\n",
    "    :param classes_per_user: number of classes assigned to each client\n",
    "    :param high_prob: highest prob sampled\n",
    "    :param low_prob: lowest prob sampled\n",
    "    :return: dictionary mapping between classes and proportions, each entry refers to other client\n",
    "    \"\"\"\n",
    "    num_classes, num_samples, _ = get_num_classes_samples(dataset)\n",
    "\n",
    "    # -------------------------------------------#\n",
    "    # Divide classes + num samples for each user #\n",
    "    # -------------------------------------------#\n",
    "    # print(num_classes)\n",
    "    assert (classes_per_user * num_users) % num_classes == 0, \"equal classes appearance is needed\"\n",
    "    count_per_class = (classes_per_user * num_users) // num_classes\n",
    "    class_dict = {}\n",
    "    for i in range(num_classes):\n",
    "        probs=np.array([1]*count_per_class)\n",
    "        probs_norm = (probs / probs.sum()).tolist()\n",
    "        class_dict[i] = {'count': count_per_class, 'prob': probs_norm}\n",
    "    # -------------------------------------#\n",
    "    # Assign each client with data indexes #\n",
    "    # -------------------------------------#\n",
    "    class_partitions = defaultdict(list)\n",
    "    for i in range(num_users):\n",
    "        c = []\n",
    "        for _ in range(classes_per_user):\n",
    "            class_counts = [class_dict[i]['count'] for i in range(num_classes)]\n",
    "            max_class_counts = np.where(np.array(class_counts) == max(class_counts))[0]\n",
    "            max_class_counts = np.setdiff1d(max_class_counts, np.array(c))\n",
    "            c.append(np.random.choice(max_class_counts))\n",
    "            class_dict[c[-1]]['count'] -= 1\n",
    "        class_partitions['class'].append(c)\n",
    "        class_partitions['prob'].append([class_dict[i]['prob'].pop() for i in c])\n",
    "    return class_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_split(dataset, num_users, class_partitions):\n",
    "    \"\"\"\n",
    "    divide data indexes for each client based on class_partition\n",
    "    :param dataset: pytorch dataset object (train/val/test)\n",
    "    :param num_users: number of clients\n",
    "    :param class_partitions: proportion of classes per client\n",
    "    :return: dictionary mapping client to its indexes\n",
    "    \"\"\"\n",
    "    num_classes, num_samples, data_labels_list = get_num_classes_samples(dataset)\n",
    "\n",
    "    # -------------------------- #\n",
    "    # Create class index mapping #\n",
    "    # -------------------------- #\n",
    "    data_class_idx = {i: np.where(data_labels_list == i)[0] for i in range(num_classes)}\n",
    "\n",
    "    # --------- #\n",
    "    # Shuffling #\n",
    "    # --------- #\n",
    "    for data_idx in data_class_idx.values():\n",
    "        random.shuffle(data_idx)\n",
    "\n",
    "    # ------------------------------ #\n",
    "    # Assigning samples to each user #\n",
    "    # ------------------------------ #\n",
    "    user_data_idx = [[] for i in range(num_users)]\n",
    "    for usr_i in range(num_users):\n",
    "        for c, p in zip(class_partitions['class'][usr_i], class_partitions['prob'][usr_i]):\n",
    "            end_idx = int(num_samples[c] * p)\n",
    "            user_data_idx[usr_i].extend(data_class_idx[c][:end_idx])\n",
    "            data_class_idx[c] = data_class_idx[c][end_idx:]\n",
    "        if len(user_data_idx[usr_i])%2 == 1: user_data_idx[usr_i] = user_data_idx[usr_i][:-1]\n",
    "\n",
    "    return user_data_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_loaders(data_name, data_path, num_users, bz, num_classes_per_user, num_classes, preprocess=None):\n",
    "    \"\"\"\n",
    "    generates train/val/test loaders of each client\n",
    "    :param data_name: name of dataset, choose from [mnist10, fashionmnist, chmnist]\n",
    "    :param data_path: root path for data dir\n",
    "    :param num_users: number of clients\n",
    "    :param bz: batch size\n",
    "    :param classes_per_user: number of classes assigned to each client\n",
    "    :return: train/val/test loaders of each client, list of pytorch dataloaders\n",
    "    \"\"\"\n",
    "    loader_params = {\"batch_size\": bz, \"shuffle\": False, \"pin_memory\": True, \"num_workers\": 0}\n",
    "    dataloaders = []\n",
    "    datasets = get_datasets(data_name, data_path, preprocess=preprocess)\n",
    "    # print(datasets)\n",
    "    cls_partitions = None\n",
    "    distribution = np.zeros((num_users, num_classes))\n",
    "    for i, d in enumerate(datasets):\n",
    "        if i == 0:\n",
    "            cls_partitions = gen_classes_per_node(d, num_users, num_classes_per_user)\n",
    "            # print(cls_partitions)\n",
    "            for index in range(num_users):\n",
    "                distribution[index][cls_partitions['class'][index]] = cls_partitions['prob'][index]\n",
    "\n",
    "            loader_params['shuffle'] = True\n",
    "        usr_subset_idx = gen_data_split(d, num_users, cls_partitions)\n",
    "\n",
    "        subsets = list(map(lambda x: torch.utils.data.Subset(d, x), usr_subset_idx))\n",
    "        dataloaders.append(list(map(lambda x: torch.utils.data.DataLoader(x, **loader_params), subsets)))\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Training and Test Data:\n",
    "\n",
    "* Preprocess and save both the training and testing data, ensuring they are correctly partitioned for client usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the training data and test data for each user right now. \n",
    "train_dataloaders, test_dataloaders  = gen_random_loaders(DATA_NAME, root, NUM_CLIENTS, BATCH_SIZE, NUM_CLASES_PER_CLIENT, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveClientData(NUM_CLIENTS, dataset, folder_name, train = True):\n",
    "\n",
    "    if train:\n",
    "        train = 'train'\n",
    "    else: \n",
    "        train = 'test'\n",
    "\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        data_directory = f'./{folder_name}/client_{train}_data'\n",
    "        os.makedirs(data_directory, exist_ok = True)\n",
    "\n",
    "        user_batch_data = []\n",
    "        user_batch_labels = []\n",
    "        for img, lab in dataset[i]:\n",
    "            user_batch_data.append(img)\n",
    "            user_batch_labels.append(lab)\n",
    "\n",
    "        user_data_tensor = torch.cat(user_batch_data, dim=0)\n",
    "        user_labels_tensor = torch.cat(user_batch_labels, dim=0)\n",
    "\n",
    "        # Save the concatenated data and labels to a single file\n",
    "        torch.save({'images': user_data_tensor, 'labels': user_labels_tensor}, f'{data_directory}/client_{train}_{i:02}.pt')\n",
    "\n",
    "\n",
    "def saveWeights(users, folder_name):\n",
    "    weight_directory = f'./{folder_name}/client_model_weights'\n",
    "    os.makedirs(weight_directory, exist_ok=True)\n",
    "    for i in range(len(users)):\n",
    "        # print('saving the weights of users:')\n",
    "        torch.save(users[i].get_model_state_dict(), f\"{weight_directory}/weight_user{i:02}.pth\")\n",
    "\n",
    "def saveModels(users, folder_name):\n",
    "    model_directory = f'./{folder_name}/client_model'\n",
    "    os.makedirs(model_directory, exist_ok = True)\n",
    "    for i in range(len(users)):\n",
    "        # print('saving the model of users:')\n",
    "        torch.save(users[i].model.state_dict(), f\"{model_directory}/model_user{i:02}.pth\")\n",
    "\n",
    "\n",
    "folder_name = 'FL_LDP_data'\n",
    "saveClientData(NUM_CLIENTS, train_dataloaders, folder_name, train = True)\n",
    "saveClientData(NUM_CLIENTS, test_dataloaders, folder_name, train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Main Model:\n",
    "\n",
    "* Implement the neural network architecture for training on both client and server sides. This model will be initialized and trained by each client individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS \n",
    "class mnist_fully_connected(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(mnist_fully_connected, self).__init__()\n",
    "        self.hidden1 = 600\n",
    "        self.hidden2 = 100\n",
    "        self.fc1 = nn.Linear(28 * 28, self.hidden1, bias=False)\n",
    "        self.relu_ = nn.ReLU(inplace=False)\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(self.hidden2, num_classes, bias=False)\n",
    "        \n",
    "    def forward(self,x, return_probs=True):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu_(self.fc1(x))\n",
    "        x = relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        if return_probs:\n",
    "            return logits, softmax(logits, dim = 1)\n",
    "        else:\n",
    "            return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Client and Server-Side Classes:\n",
    "\n",
    "* Define classes for the client and server models. Each client will have a local instance of the model, and the server will maintain the central model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDPUser:\n",
    "    def __init__(self, index, device, model, n_classes, input_shape, train_dataloader, epochs, rounds, \n",
    "                 target_epsilon, target_delta, sr, max_norm=2.0, disc_lr=5e-1, mp_bs = 3):\n",
    "        self.index = index\n",
    "        self.rounds = rounds\n",
    "        self.target_epsilon = target_epsilon\n",
    "        self.epsilon = 0\n",
    "        self.delta = target_delta\n",
    "        self.model = model(num_classes=n_classes)\n",
    "        self.model = ModuleValidator.fix(self.model)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.sr = sr\n",
    "        self.mp_bs = mp_bs\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.disc_lr = disc_lr\n",
    "        self.acc_metric = torchmetrics.Accuracy().to(device)\n",
    "        self.device = device\n",
    "        self.max_norm= max_norm\n",
    "        self.epochs = epochs\n",
    "        self.optim = torch.optim.SGD(self.model.parameters(), self.disc_lr)\n",
    "        self.make_local_private()\n",
    "        \n",
    "\n",
    "\n",
    "    def make_local_private(self):\n",
    "        self.privacy_engine = opacus.PrivacyEngine()\n",
    "        self.model, self.optim, self.train_dataloader = self.privacy_engine.make_private_with_epsilon(module=self.model, optimizer=self.optim,\n",
    "                                                                                                      data_loader=self.train_dataloader, epochs=self.epochs*self.rounds*self.sr,\n",
    "                                                                                                      target_epsilon=self.target_epsilon, target_delta=self.delta,\n",
    "                                                                                                      max_grad_norm=self.max_norm)\n",
    "\n",
    "    def train(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            with BatchMemoryManager(data_loader=self.train_dataloader, max_physical_batch_size=self.mp_bs, optimizer=self.optim) as batch_loader:\n",
    "                for images, labels in batch_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    self.optim.zero_grad()\n",
    "                    logits, preds = self.model(images, return_probs=True)\n",
    "                    loss = self.loss_fn(logits, labels)\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "                    self.acc_metric(preds, labels)\n",
    "        self.epsilon = self.privacy_engine.get_epsilon(self.delta)\n",
    "        print(f\"Client: {self.index} ACC: {self.acc_metric.compute()}, episilon: {self.epsilon}\")\n",
    "        self.acc_metric.reset()\n",
    "        self.model.to('cpu')\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        testing_corrects = 0\n",
    "        testing_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                _, preds = self.model(images, return_probs=True)\n",
    "                testing_corrects += torch.sum(torch.argmax(preds, dim=1) == labels)\n",
    "                testing_sum += len(labels)\n",
    "        self.model.to('cpu')\n",
    "        return testing_corrects.cpu().detach().numpy(), testing_sum\n",
    "\n",
    "    def get_model_state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def set_model_state_dict(self, weights):\n",
    "        for key, value in self.model.state_dict().items():\n",
    "            if 'bn' not in key:\n",
    "                self.model.state_dict()[key].data.copy_(weights[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDPServer:\n",
    "    def __init__(self, device, model, n_classes, input_shape, noise_multiplier=1, sample_clients=10, disc_lr=1):\n",
    "        self.model = model(num_classes=n_classes)\n",
    "        self.model = ModuleValidator.fix(self.model)\n",
    "        self.privacy_engine = opacus.PrivacyEngine()\n",
    "        self.model = self.privacy_engine._prepare_model(self.model)\n",
    "        self.device = device\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.sample_clients = sample_clients \n",
    "        self.disc_lr = disc_lr\n",
    "\n",
    "    def get_model_state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "# Get the average weight of the client models.\n",
    "def agg_weights(weights):\n",
    "    with torch.no_grad():\n",
    "        weights_avg = copy.deepcopy(weights[0])\n",
    "        for k in weights_avg.keys():\n",
    "            for i in range(1, len(weights)):\n",
    "                weights_avg[k] += weights[i][k]\n",
    "            weights_avg[k] = torch.div(weights_avg[k], len(weights))\n",
    "    return weights_avg\n",
    "\n",
    "def evaluate_global(users, test_dataloders, users_index):\n",
    "    testing_corrects = 0\n",
    "    testing_sum = 0\n",
    "    for index in users_index:\n",
    "        corrects, num = users[index].evaluate(test_dataloders[index])\n",
    "        testing_corrects += corrects\n",
    "        testing_sum += num\n",
    "    print(f\"Acc: {testing_corrects / testing_sum}\")\n",
    "    return (testing_corrects / testing_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Weights from Server to Clients:\n",
    "\n",
    "* Synchronize the models by passing the initial server-side model weights to all clients, ensuring all clients start training with the same model structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_obj = LDPUser\n",
    "server_obj = LDPServer\n",
    "MODEL = mnist_fully_connected\n",
    "server_obj = LDPServer\n",
    "server = server_obj(device, MODEL, NUM_CLASSES, None, **server_param)\n",
    "users = [user_obj(i, device, MODEL, NUM_CLASSES, None, train_dataloaders[i], **user_param) for i in range(NUM_CLIENTS)]\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    users[i].set_model_state_dict(server.get_model_state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Aggregation Loop:\n",
    "\n",
    "* Train the models on the client side and periodically send the updated model parameters back to the server.\n",
    "* The server will aggregate these parameters from all clients (e.g., using weighted averaging) and send the updated global model back to the clients to continue the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "for round in range(ROUNDS): # Changed to one for practice. ROUND\n",
    "\n",
    "    random_index = np.random.choice(NUM_CLIENTS, int(sample_rate*NUM_CLIENTS), replace=False)\n",
    "    for index in random_index:\n",
    "        users[index].train() # training the user for each round.\n",
    "    # for index in random_index:users[index].set_model_state_dict2(torch.load(client_weights[index]))\n",
    "\n",
    "    # Saving the models and the weight in the last round\n",
    "    if round == ROUNDS - 1:\n",
    "        saveModels(users, folder_name)\n",
    "        saveWeights(users, folder_name)\n",
    "    \n",
    "    if MODE == \"LDP\":\n",
    "        weights_agg = agg_weights([users[index].get_model_state_dict() for index in random_index])\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            users[i].set_model_state_dict(weights_agg)\n",
    "    else:\n",
    "        server.agg_updates([users[index].get_model_state_dict() for index in random_index])\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            users[i].set_model_state_dict(server.get_model_state_dict())\n",
    "\n",
    "    print(f\"Round: {round+1}\")\n",
    "    acc = evaluate_global(users, test_dataloaders, range(NUM_CLIENTS))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    if MODE == \"LDP\":\n",
    "        eps = max([user.epsilon for user in users])\n",
    "        print(f\"Epsilon: {eps}\")\n",
    "        if eps > target_epsilon:\n",
    "            saveModels(users, folder_name)\n",
    "            saveWeights(users, folder_name)\n",
    "            break\n",
    "\n",
    "print('Federated Learning Client Training Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Model and Data:\n",
    "\n",
    "* After training, visualize the saved model and data for one of the clients to better understand the learning process. This will provide insights into the accuracy, privacy effects, and model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.load('./FL_LDP_data/client_train_data/client_train_00.pt')\n",
    "testing_data = torch.load('./FL_LDP_data/client_test_data/client_test_00.pt')\n",
    "\n",
    "x_train = training_data['images']\n",
    "y_train = training_data['labels']\n",
    "x_test = testing_data['images']\n",
    "y_test = testing_data['labels']\n",
    "# convert the labels to one_hot_encoding\n",
    "num_classes = len(torch.unique(y_train))\n",
    "\n",
    "print(f\"Train data shape: {x_train.shape}, training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}, test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_prefix(state_dict, prefix=\"_module.\"):\n",
    "    \"\"\"\n",
    "    Strip a prefix from the state_dict keys.\n",
    "    Args:\n",
    "        state_dict (dict): The state_dict with the potentially prefixed keys.\n",
    "        prefix (str): The prefix to remove.\n",
    "    Returns:\n",
    "        dict: The state_dict with the prefix removed from the keys.\n",
    "    \"\"\"\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(prefix):\n",
    "            new_state_dict[k[len(prefix):]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './FL_LDP_data/client_model_weights/weight_user00.pth'  # Path to your saved model weights\n",
    "model = mnist_fully_connected(num_classes)\n",
    "\n",
    "# Load the state_dict\n",
    "state_dict = torch.load(model_path)\n",
    "# Strip the \"_module.\" prefix if it exists\n",
    "state_dict = strip_prefix(state_dict, prefix=\"_module.\")\n",
    "# Load the modified state_dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "# Set the model to evaluation mode\n",
    "\n",
    "# Check the models accuracy on the test data\n",
    "with torch.no_grad():\n",
    "    test_logits, _ = model(x_test)\n",
    "    \n",
    "prediction = torch.sum(torch.argmax(test_logits, axis = 1) == y_test) / len(y_test)\n",
    "print('The accuray on the test data is : ', prediction.numpy() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the train data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5, figsize=(15, 7))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, (image, label) in enumerate(zip(x_train[:10], y_train[:10])):\n",
    "  img = image.permute(1, 2, 0)\n",
    "  ax[i].imshow(img, cmap = \"Greys\")\n",
    "  ax[i].set_title(f\"Label: {label}\",  fontsize=12)\n",
    "  ax[i].set_xticks([])\n",
    "  ax[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the test data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5, figsize=(15, 7))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, (image, label) in enumerate(zip(x_test[:10], y_test[:10])):\n",
    "  img = image.permute(1, 2, 0)\n",
    "  ax[i].imshow(img, cmap = \"Greys\")\n",
    "  ax[i].set_title(f\"Label: {label}\",  fontsize=12)\n",
    "  ax[i].set_xticks([])\n",
    "  ax[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "This workshop focuses on training a federated learning model with 100 clients using differential privacy to ensure data security. Participants will install necessary libraries like PyTorch and Opacus, divide the MNIST dataset among clients, build client and server models, and perform privacy-preserving training. The process includes model synchronization, aggregation, and visualization of the results to demonstrate privacy and model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privatefl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
